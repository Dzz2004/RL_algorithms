<!DOCTYPE html>
<html>
<head>
<title>REPORT.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AE%9E%E8%B7%B5%E6%8A%A5%E5%91%8A"><strong>强化学习算法实践报告</strong></h1>
<blockquote>
<p><strong>姓名：董壮志</strong><br>
<strong>学号：1120221602</strong><br>
<strong>教师：李长升</strong><br>
<strong>课程：周四11-13节</strong><br>
<strong>学期：2024-2025第一学期</strong> <br>
<strong>算法实现</strong></p>
<ul>
<li><input type="checkbox" id="checkbox0" checked="true"><label for="checkbox0">蒙特卡洛策略评估（MC）</label></li>
<li><input type="checkbox" id="checkbox1" checked="true"><label for="checkbox1">动态规划方法(DP)</label></li>
<li><input type="checkbox" id="checkbox2" checked="true"><label for="checkbox2">Sarsa算法</label></li>
<li><input type="checkbox" id="checkbox3" checked="true"><label for="checkbox3">Q学习(Q-Learning)</label></li>
<li><input type="checkbox" id="checkbox4" checked="true"><label for="checkbox4">策略梯度（PG）</label></li>
<li><input type="checkbox" id="checkbox5" checked="true"><label for="checkbox5">近端策略优化（PPO）</label></li>
<li><input type="checkbox" id="checkbox6" checked="true"><label for="checkbox6">深度Q网络（DQN）</label></li>
<li><input type="checkbox" id="checkbox7" checked="true"><label for="checkbox7">演员-评论家（A2C）</label></li>
</ul>
</blockquote>
<h2 id="%E4%B8%80%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%96%B9%E6%B3%95">一、表格型方法</h2>
<h3 id="1-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0mc">1. 蒙特卡洛策略评估（MC）</h3>
<h4 id="%E5%8E%9F%E7%90%86">原理</h4>
<p>蒙特卡洛方法的核心思想是利用多次随机实验的结果来逼近某个目标函数的期望值。在强化学习问题上，蒙特卡洛方法通过观察完整的序列来估计动作值函数 ${ Q(s, a) }$或状态值函数 ${ V(s) }$。</p>
<h4 id="%E7%89%B9%E7%82%B9">特点</h4>
<ul>
<li>无模型：不需要环境的模型知识，只需要通过采样进行评估。</li>
<li>大数定律：随着采样次数的增加，估计结果会收敛于真实值。</li>
<li>样本效率：对于非马尔可夫或复杂环境可能较低，因为有限样本数量下的估计可能不准确。</li>
</ul>
<h4 id="%E5%AE%9E%E6%96%BD%E6%AD%A5%E9%AA%A4">实施步骤</h4>
<p>以下是使用蒙特卡洛方法进行策略评估的基本步骤：</p>
<ul>
<li>初始化：初始化状态值函数 ${ V(s) }$ ，或动作值函数 ${ Q(s, a) }$ 。</li>
<li>生成序列：</li>
<li>使用当前策略 ${ \pi }$ 生成一个完整的序列，直到达到终止状态。</li>
<li>记录在序列中每个状态、动作和奖励。</li>
<li>计算回报：
对于每个序列中的状态，计算从该状态以后获得的总回报 ${ G_t }$。</li>
<li>更新估计：
更新估计的值函数： ${ V(s) \leftarrow V(s) + \alpha (G_t - V(s)) }$，其中 ${\alpha}$ 是学习率。</li>
<li>重复：重复生成序列和更新步骤，直到值函数收敛。</li>
</ul>
<h4 id="%E7%AD%96%E7%95%A5%E6%94%B9%E8%BF%9B">策略改进</h4>
<p>一旦有了值函数的良好估计，策略也可以进行改进。通过以下步骤，实现策略改进：
对于贪心策略的选取：选择使动作值最大化的动作。
使用 ${ \epsilon }$-贪心法探索，以确保策略的持续改进：
$${ \pi(a|s) = \begin{cases} 1 - \epsilon + \frac{\epsilon}{|\mathcal{A}(s)|} &amp; \text{if } a = \arg\max_a Q(s, a) \ \frac{\epsilon}{|\mathcal{A}(s)|} &amp; \text{otherwise} \end{cases} }$$<br>
其中 ${ \epsilon }$ 是探索率。</p>
<h4 id="%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</h4>
<ul>
<li>离散化： 对连续的状态空间进行离散化，以便于处理马尔可夫决策过程。其中离散化系数，也就是对连续区间进行等间距分隔的“箱子”个数num_bins作为训练时的超参数。<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_bins</span><span class="hljs-params">(num_bins, lower_bound, upper_bound)</span>:</span>
  <span class="hljs-keyword">return</span> np.linspace(lower_bound, upper_bound, num_bins)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">discretize_state</span><span class="hljs-params">(state, cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins)</span>:</span>
  cart_pos_idx = np.digitize(state[<span class="hljs-number">0</span>], cart_pos_bins) - <span class="hljs-number">1</span>
  cart_vel_idx = np.digitize(state[<span class="hljs-number">1</span>], cart_vel_bins) - <span class="hljs-number">1</span>
  pole_angle_idx = np.digitize(state[<span class="hljs-number">2</span>], pole_angle_bins) - <span class="hljs-number">1</span>
  pole_vel_idx = np.digitize(state[<span class="hljs-number">3</span>], pole_vel_bins) - <span class="hljs-number">1</span>
  <span class="hljs-keyword">return</span> (cart_pos_idx, cart_vel_idx, pole_angle_idx, pole_vel_idx)
</div></code></pre>
</li>
<li>主学习函数
<ul>
<li>状态空间离散化：首先通过 create_bins 分别将小车位置、速度、杆角度和杆速度离散化。</li>
<li>初始Q表和返回计数：初始化 Q_table 用于存储状态动作值，returns_count 用于计算回报次数均值。</li>
<li>每个episode的初始化：重置环境，清空当前episode的状态、动作和奖励。</li>
<li>随机策略选择动作：在每步选择随机动作，并执行动作获得新状态和奖励，随后将其存储。</li>
<li>计算蒙特卡洛回报 ${G}$ 并更新Q表：从后向前遍历每个episode's (状态, 动作)，计算累计折扣回报，更新Q值。</li>
</ul>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">monte_carlo_learning</span><span class="hljs-params">(env, num_episodes, num_bins, gamma=<span class="hljs-number">1.0</span>)</span>:</span>
  <span class="hljs-comment"># 为CartPole特性创建bin</span>
  cart_pos_bins = create_bins(num_bins, <span class="hljs-number">-2.4</span>, <span class="hljs-number">2.4</span>)
  cart_vel_bins = create_bins(num_bins, <span class="hljs-number">-3.0</span>, <span class="hljs-number">3.0</span>)
  pole_angle_bins = create_bins(num_bins, <span class="hljs-number">-0.5</span>, <span class="hljs-number">0.5</span>)
  pole_vel_bins = create_bins(num_bins, <span class="hljs-number">-2.0</span>, <span class="hljs-number">2.0</span>)

  <span class="hljs-comment"># 初始化策略和Q-table</span>
  action_space_size = env.action_space.n
  Q_table = np.zeros((num_bins, num_bins, num_bins, num_bins, action_space_size))
  returns_count = np.zeros((num_bins, num_bins, num_bins, num_bins, action_space_size))

  <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> range(num_episodes):
      state = env.reset()
      episode_states_actions_rewards = []
      done = <span class="hljs-literal">False</span>

      <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
          state_discrete = discretize_state(state, cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins)
          action = np.random.choice(env.action_space.n)  <span class="hljs-comment"># 随即选择动作</span>

          <span class="hljs-comment"># 执行动作，观察结果</span>
          next_state, reward, done, _ = env.step(action)
          episode_states_actions_rewards.append((state_discrete, action, reward))
          state = next_state

      G = <span class="hljs-number">0</span>
      <span class="hljs-keyword">for</span> state_discrete, action, reward <span class="hljs-keyword">in</span> reversed(episode_states_actions_rewards):
          G = gamma * G + reward
          sa_pair = (*state_discrete, action)
          returns_count[sa_pair] += <span class="hljs-number">1</span>
          Q_table[sa_pair] += (G-Q_table[sa_pair]) / returns_count[sa_pair]

      <span class="hljs-keyword">if</span> episode % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:
          print(<span class="hljs-string">f"Episode <span class="hljs-subst">{episode}</span> complete."</span>)

  <span class="hljs-keyword">return</span> Q_table
</div></code></pre>
</li>
<li>提取策略函数：从Q表中提取策略：对每个状态选择使Q值最大的动作。<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_policy</span><span class="hljs-params">(Q_table)</span>:</span>
  <span class="hljs-keyword">return</span> np.argmax(Q_table, axis=<span class="hljs-number">-1</span>)
</div></code></pre>
</li>
<li>测试函数<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">(env,policy,num_bins)</span>:</span>
  state = env.reset()
  cart_pos_bins = create_bins(num_bins, <span class="hljs-number">-2.4</span>, <span class="hljs-number">2.4</span>)
  cart_vel_bins = create_bins(num_bins, <span class="hljs-number">-3.0</span>, <span class="hljs-number">3.0</span>)
  pole_angle_bins = create_bins(num_bins, <span class="hljs-number">-0.5</span>, <span class="hljs-number">0.5</span>)
  pole_vel_bins = create_bins(num_bins, <span class="hljs-number">-2.0</span>, <span class="hljs-number">2.0</span>)

  avg_reward = <span class="hljs-number">0</span>
  num_test_episodes = <span class="hljs-number">100</span>
  rewards = []
  <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> range(num_test_episodes):
      state = env.reset()
      episode_reward = <span class="hljs-number">0</span>
      done = <span class="hljs-literal">False</span>
      <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
          state_discrete = discretize_state(state, cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins)
          action = policy[state_discrete]
          state, reward, done, _ = env.step(action)
          episode_reward += reward
      rewards.append(episode_reward)
      avg_reward += episode_reward
      <span class="hljs-keyword">if</span> episode % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
          print(<span class="hljs-string">f"test: Episode <span class="hljs-subst">{episode}</span> reward: <span class="hljs-subst">{episode_reward}</span>"</span>)
  avg_reward /= num_test_episodes
  print(<span class="hljs-string">f"Average reward: <span class="hljs-subst">{avg_reward}</span>"</span>)
  <span class="hljs-keyword">return</span> avg_reward, rewards
</div></code></pre>
</li>
</ul>
<h4 id="%E5%8F%82%E6%95%B0%E5%AE%9E%E9%AA%8C">参数实验</h4>
<ul>
<li>
<p>固定gamma，测试bin</p>
<p><img src="MC_results/num_bins-average_reward.png" alt="alt text"></p>
<p><strong>可以发现，当gamma=1时，bins越大，即离散化程度越大，100次测试下平均奖励在逐渐变小</strong></p>
</li>
<li>
<p>固定bin，测试gamma</p>
<p><img src="MC_results/gamma-average_reward.png" alt="alt text"></p>
<p><strong>可以发现，当bin=5时，gamma取0.5、0.6为宜。</strong></p>
</li>
</ul>
<h4 id="%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C">最终结果</h4>
<ul>
<li>
<p>当gamma=0.5，bin=5，train_episodes=6000时，结果如下：</p>
<p><img src="MC_results/gamma=0.5,epi=6000,bin=5.png" alt="alt text"></p>
<p><strong>平均奖励来到了487.53，基本接近于该环境的奖励上限。但是，在测试中，仍有不同程度的震荡，即不能更好的稳定在500左右。</strong></p>
</li>
</ul>
<h4 id="%E7%BB%93%E8%AE%BA">结论</h4>
<ul>
<li>蒙特卡洛方法是一种无模型的强化学习方法，通过多次采样来估计值函数。</li>
<li>蒙特卡洛方法的优点是简单易懂，不需要环境模型，但需要大量的采样。</li>
<li>蒙特卡洛方法在实际问题中往往适用于小规模、有完整模型信息的问题。对于大型、复杂、模型不确定的问题，往往还需借助其他方法如近似动态规划或强化学习。</li>
</ul>
<h3 id="2%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95dp">2.动态规划方法(DP)</h3>
<h4 id="%E5%8E%9F%E7%90%86">原理</h4>
<p>动态规划（Dynamic Programming, DP）是一类用于解决复杂问题的优化算法，适用于可以分解为有重叠子问题的情形。它广泛用于各类最优化和决策问题中，尤其是在已知数学模型的强化学习中。DP通过系统地枚举所有可能状态和动作对，从已有信息中不断进行改进，最终逼近最优解。</p>
<h4 id="%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E5%9C%A8%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8">动态规划在强化学习中的应用</h4>
<p>在强化学习中，DP方法通常用于求解马尔可夫决策过程（MDP）。它依赖于以下假设：</p>
<ul>
<li>完全模型已知：知道所有状态转移概率 ${ P(s' | s, a) }$ 和相应的奖励函数 ${ R(s, a, s') }$。</li>
<li>有界状态空间和动作空间：通常需要有限数量的状态和动作以保证计算的可行性。
动态规划的基本原理</li>
<li>两个核心算法：策略迭代（Policy Iteration）和价值迭代（Value Iteration）。本次例子中使用的是价值迭代算法。</li>
</ul>
<h4 id="%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3">价值迭代</h4>
<ul>
<li>价值迭代的核心公式：
基于贝尔曼最优方程，价值迭代不断更新每个状态的价值函数 ${ V(s) }$ ，以期望累积回报最大化为目标： ${ V(s) = \max_a \sum_{s'} P(s' | s, a) [R(s, a, s') + \gamma V(s') ]}$ 其中，${\gamma}$ 是折扣因子，用于平衡即期和远期奖励。</li>
<li>更新过程：
初始化状态值 ${ V(s) }$ 为零或任意猜测值。
对于每个状态 ${ s }$，计算在所有可能动作 ${ a }$ 的期望价值并选取最大者更新 ${ V(s) }$。
重复上述过程直到 ${ V(s) }$ 收敛于某一稳定值，即状态值变化成无动态改变（差值小于某阈值 ${\theta}$ 时停止）。</li>
<li>从价值函数提取策略：
一旦价值函数 ${V(s) }$ 收敛，通过选择在给定状态下使得价值函数达到最大值的动作提取最佳策略。</li>
</ul>
<h4 id="%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E7%9A%84%E4%BC%98%E7%82%B9%E4%B8%8E%E7%BC%BA%E7%82%B9">动态规划的优点与缺点</h4>
<ul>
<li>优点：
<ul>
<li>在已知模型时能够找到全局最优解。</li>
<li>理论证明了收敛性和最优性。</li>
</ul>
</li>
<li>缺点：
<ul>
<li>需知道完整的环境模型，而这在实际问题中往往不可用。</li>
<li>在状态空间和动作空间过大时，计算和存储负担极重（“维数灾难”）。</li>
<li>在实践中，动态规划适用于小规模、有完整模型信息的问题。对于大型、复杂、模型不确定的问题，往往还需借助其他方法如近似动态规划或强化学习。</li>
</ul>
</li>
</ul>
<h4 id="%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</h4>
<ul>
<li>
<p>离散化：跟上一个算法一样，这里略去。</p>
<ul>
<li>Q表跟新：更新Q表的过程是通过迭代更新每个状态的价值函数，直到价值函数收敛。<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">q_value_iteration</span><span class="hljs-params">(env, num_bins, gamma=<span class="hljs-number">1.0</span>, theta=<span class="hljs-number">1e-5</span>)</span>:</span>
  cart_pos_bins = create_bins(num_bins, <span class="hljs-number">-2.4</span>, <span class="hljs-number">2.4</span>)
  cart_vel_bins = create_bins(num_bins, <span class="hljs-number">-3.0</span>, <span class="hljs-number">3.0</span>)
  pole_angle_bins = create_bins(num_bins, <span class="hljs-number">-0.5</span>, <span class="hljs-number">0.5</span>)
  pole_vel_bins = create_bins(num_bins, <span class="hljs-number">-2.0</span>, <span class="hljs-number">2.0</span>)

  action_space_size = env.action_space.n
  Q = np.zeros((num_bins, num_bins, num_bins, num_bins, action_space_size))
  i = <span class="hljs-number">0</span>
  <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
      delta = <span class="hljs-number">0</span>
      print(<span class="hljs-string">f"Iteration<span class="hljs-subst">{i}</span>"</span>)
      <span class="hljs-keyword">for</span> cart_pos_idx <span class="hljs-keyword">in</span> range(num_bins):
          <span class="hljs-keyword">for</span> cart_vel_idx <span class="hljs-keyword">in</span> range(num_bins):
              <span class="hljs-keyword">for</span> pole_angle_idx <span class="hljs-keyword">in</span> range(num_bins):
                  <span class="hljs-keyword">for</span> pole_vel_idx <span class="hljs-keyword">in</span> range(num_bins):
                      <span class="hljs-keyword">for</span> action <span class="hljs-keyword">in</span> range(action_space_size):
                          state_values = []
                          env.reset()
                          env.env.state = (cart_pos_bins[cart_pos_idx],
                                           cart_vel_bins[cart_vel_idx],
                                           pole_angle_bins[pole_angle_idx],
                                           pole_vel_bins[pole_vel_idx])
                          next_state, reward, done, _ = env.step(action)
                          <span class="hljs-keyword">if</span> done:
                              Q_value = reward
                          <span class="hljs-keyword">else</span>:
                              next_state_discrete = discretize_state(next_state, cart_pos_bins, cart_vel_bins,
                                                                     pole_angle_bins, pole_vel_bins)
                              Q_value = reward + gamma * np.max(Q[next_state_discrete])

                          state_values.append(Q_value)

                          old_value = Q[cart_pos_idx, cart_vel_idx, pole_angle_idx, pole_vel_idx, action]
                          Q[cart_pos_idx, cart_vel_idx, pole_angle_idx, pole_vel_idx, action] = Q_value
                          delta = max(delta, np.abs(old_value - Q_value))

      <span class="hljs-keyword">if</span> delta &lt; theta:
          <span class="hljs-keyword">break</span>
      <span class="hljs-keyword">else</span>:
          print(<span class="hljs-string">f"iteration <span class="hljs-subst">{i}</span> complete,delta=<span class="hljs-subst">{delta}</span>"</span>)
          i += <span class="hljs-number">1</span>

  policy = np.argmax(Q, axis=<span class="hljs-number">-1</span>)
  <span class="hljs-keyword">return</span> policy, Q
</div></code></pre>
</li>
</ul>
</li>
<li>
<p>测试函数： 与上一个算法一样，这里略去。</p>
</li>
<li>
<p>参数实验：</p>
</li>
<li>
<p>固定gamma，测试bin</p>
<p><img src="DP_results/Average reward vs num_bins.png" alt="alt text"></p>
<p><strong>可以发现，当gamma=0.5，theta=1e-10时，离散化系数为5最优</strong></p>
</li>
<li>
<p>固定bin，测试gamma</p>
<p><img src="DP_results/Average reward vs gamma.png" alt="alt text"></p>
</li>
</ul>
<p><strong>可以发现，当bin=5，theta=1e-10时，gamma取0.8最优</strong></p>
<h4 id="%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C">最终结果</h4>
<ul>
<li>
<p>当gamma=0.8，bin=5，theta=1e-10时，结果如下：</p>
<p><img src="DP_results/gamma=0.5，bins=5，avg_reward=50.6.png" alt="alt text"></p>
<p><strong>可以看到效果并不理想。</strong></p>
</li>
</ul>
<h4 id="%E5%8F%8D%E6%80%9D">反思</h4>
<ul>
<li>由于DP方法需要完整的环境模型,也就是完整的状态转移矩阵，而在实际问题中，这种情况往往不可用。因此，DP方法在实际问题中的应用受到了很大的限制。</li>
<li>在实际问题中，DP方法往往适用于小规模、有完整模型信息的问题。对于大型、复杂、模型不确定的问题，往往还需借助其他方法如近似动态规划或强化学习。</li>
</ul>
<h3 id="3-sarsa%E7%AE%97%E6%B3%95">3. Sarsa算法</h3>
<h4 id="%E5%8E%9F%E7%90%86">原理</h4>
<p>SARSA（State-Action-Reward-State-Action）是一种流行的强化学习算法，属于基于Q值的算法，用于解决马尔可夫决策过程（MDP）问题。它是一种在线、基于策略的学习算法，主要用于寻找状态-动作对的价值函数，从而帮助智能体在环境中学习最佳策略。</p>
<p>SARSA算法的基本原理如下：</p>
<ul>
<li>
<p>初始化：初始化任意的Q值函数 ${ Q(s, a) }$，通常Q值表中的值可以设为零或小的随机数，初始化最初的状态。</p>
</li>
<li>
<p>选择策略：通过策略选择动作（例如 ε-greedy）。</p>
<ul>
<li>ε-greedy策略：大部分时候选择当前已知Q值最大的动作（利用），并且以ε的概率随机选择一个动作（探索）。
执行动作与观察：在状态 ${ s_t }$ 下执行动作 ${ a_t }$，观察获得的即时回报 ${ r_{t+1} }$ 以及下一个状态 ${ s_{t+1} }$。</li>
</ul>
</li>
<li>
<p>选择下一个动作：根据策略选择下一个动作 ${ a_{t+1} }$ 在新状态 ${ s_{t+1} }$。</p>
</li>
<li>
<p>更新Q值：</p>
<ul>
<li>使用SARSA更新规则：
$${ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)] }$$
其中， ${\alpha}$ 是学习率,${0 &lt; \alpha ≤ 1}$，${ \gamma }$ 是折扣因子,${0 ≤ \gamma ≤ 1}$。</li>
</ul>
</li>
<li>
<p>更新状态和动作：将状态和动作更新为 ${ s_{t+1} }$ 和 ${ a_{t+1} }$。</p>
</li>
<li>
<p>重复过程：继续重复上述步骤直到达到终止条件（例如，达到指定的迭代次数或环境达到终止状态）。</p>
</li>
</ul>
<p>SARSA是一种逐步更新的算法，与Q学习的最大区别在于SARSA是基于在线状态-动作对进行更新，因此会遵循智能体实际采取的动作序列，算是一种跟踪智能体策略的&quot;在线&quot;更新法。</p>
<h4 id="%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</h4>
<ul>
<li>
<p>离散化：跟上一个算法一样，这里略去。</p>
</li>
<li>
<p>动作选择：选择动作的过程是通过策略选择动作，这里使用的是ε-greedy策略。</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_action</span><span class="hljs-params">(Q, state, epsilon)</span>:</span>
  <span class="hljs-keyword">if</span> np.random.random() &lt; epsilon:
      <span class="hljs-keyword">return</span> np.random.choice([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
  <span class="hljs-keyword">else</span>:
      <span class="hljs-keyword">return</span> np.argmax(Q[state])
</div></code></pre>
</li>
<li>
<p>SARSA更新：更新Q值的过程是通过SARSA更新规则，即根据当前状态、动作、奖励、下一个状态和下一个动作更新Q值。</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sarsa</span><span class="hljs-params">(env, n_episodes, lr, gamma, epsilon, epsilon_decay, epsilon_min, number_of_bins)</span>:</span>
  n_actions = env.action_space.n

  <span class="hljs-comment"># Create bins for each dimension of the state space</span>
  cart_pos_bins = create_bins(number_of_bins, <span class="hljs-number">-4.8</span>, <span class="hljs-number">4.8</span>)
  cart_vel_bins = create_bins(number_of_bins, <span class="hljs-number">-3.5</span>, <span class="hljs-number">3.5</span>)
  pole_angle_bins = create_bins(number_of_bins, <span class="hljs-number">-0.418</span>, <span class="hljs-number">0.418</span>)
  pole_vel_bins = create_bins(number_of_bins, <span class="hljs-number">-3.5</span>, <span class="hljs-number">3.5</span>)

  <span class="hljs-comment"># Initialize Q-table</span>
  q_table = np.zeros((number_of_bins, number_of_bins, number_of_bins, number_of_bins, n_actions))

  <span class="hljs-comment"># Main SARSA loop</span>
  <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> range(n_episodes):
      state = discretize_state(env.reset(), cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins)
      done = <span class="hljs-literal">False</span>
      action = choose_action(state, q_table, n_actions, epsilon)

      <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
          next_state_raw, reward, done, _ = env.step(action)
          next_state = discretize_state(next_state_raw, cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins)
          next_action = choose_action(next_state, q_table, n_actions, epsilon)

          <span class="hljs-comment"># Update Q-table</span>
          td_target = reward + gamma * q_table[next_state][next_action] * (<span class="hljs-keyword">not</span> done)
          td_error = td_target - q_table[state][action]
          q_table[state][action] += lr * td_error

          state = next_state
          action = next_action

      <span class="hljs-comment"># Update epsilon</span>
      epsilon = max(epsilon_min, epsilon * epsilon_decay)

      <span class="hljs-keyword">if</span> (episode + <span class="hljs-number">1</span>) % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:
          print(<span class="hljs-string">f'Episode: <span class="hljs-subst">{episode + <span class="hljs-number">1</span>}</span>, Epsilon: <span class="hljs-subst">{epsilon:<span class="hljs-number">.3</span>f}</span>'</span>)

  <span class="hljs-keyword">return</span> q_table
</div></code></pre>
</li>
<li>
<p>测试函数： 与上一个算法一样，这里略去。</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">(env, q_table, number_of_bins, n_episodes=<span class="hljs-number">1000</span>)</span>:</span>
  <span class="hljs-comment"># Create bins for each dimension of the state space (should match training)</span>
  cart_pos_bins = create_bins(number_of_bins, <span class="hljs-number">-4.8</span>, <span class="hljs-number">4.8</span>)
  cart_vel_bins = create_bins(number_of_bins, <span class="hljs-number">-3.5</span>, <span class="hljs-number">3.5</span>)
  pole_angle_bins = create_bins(number_of_bins, <span class="hljs-number">-0.418</span>, <span class="hljs-number">0.418</span>)
  pole_vel_bins = create_bins(number_of_bins, <span class="hljs-number">-3.5</span>, <span class="hljs-number">3.5</span>)

  total_rewards = []

  <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(n_episodes):
      state = discretize_state(env.reset(), cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins)
      done = <span class="hljs-literal">False</span>
      total_reward = <span class="hljs-number">0</span>

      <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
          action = np.argmax(q_table[state])  <span class="hljs-comment"># Select action with the highest Q value</span>
          next_state_raw, reward, done, _ = env.step(action)
          next_state = discretize_state(next_state_raw, cart_pos_bins, cart_vel_bins, pole_angle_bins, pole_vel_bins)

          total_reward += reward
          state = next_state

      total_rewards.append(total_reward)

  average_reward = np.mean(total_rewards)
  print(<span class="hljs-string">f'Average reward over <span class="hljs-subst">{n_episodes}</span> episodes: <span class="hljs-subst">{average_reward:<span class="hljs-number">.2</span>f}</span>'</span>)
  <span class="hljs-keyword">return</span> average_reward, total_rewards
</div></code></pre>
</li>
<li>
<p>参数实验：</p>
<ul>
<li>
<p>学习率调整</p>
<p><img src="SARSA_results/Average reward vs Learning Rate.png" alt="alt text"></p>
<p><strong>可以看到，当学习率为0.3时，看似效果最好，但是反复实验之后，最佳学习率通常在0.2-0.3之间</strong></p>
</li>
<li>
<p>折扣因子调整</p>
<p><img src="SARSA_results/Average reward vs gamma.png" alt="alt text"></p>
<p><strong>可以看到，当折扣因子为0.8时，效果最好。</strong></p>
<p>CartPole环境的目标是保持杆尽可能长时间地直立，这本质上是一个长期目标问题。高折扣因子让智能体更加重视通过多步行动来保持平衡，而不是仅仅关注短期的收益。
高折扣因子鼓励智能体最大化累积奖励，意味着它会努力保持杆的平衡以获得更多的未来回报。
使用更高的折扣因子可以使策略更稳定，因为它促使智能体考虑未来多步的潜在影响，而不是过于注重当前的一步。
在众多文献和实践中，CartPole通常使用接近1的折扣因子，这被认为在这种持续的平衡任务中表现良好。</p>
</li>
<li>
<p>训练集数调整</p>
<p><strong>在上述最佳参数的基础上</strong>，为探究训练集数对实验结果的影响，我们选取了4000，5000，6000三个不同的训练集数进行实验。并将训练得到的策略和Q表格进行100次测试，并计算平均奖励。</p>
<p><strong>episode=4000：</strong></p>
<p><img src="SARSA_results/Figure_4_4000.png" alt="Figure_4_4000.png"></p>
<p><strong>episode=5000：</strong></p>
<p><img src="SARSA_results/Figure_6_5000.png" alt="Figure_4_5000.png"></p>
<p><strong>episode=6000：</strong></p>
<p><img src="SARSA_results/Figure_5_6000.png" alt="Figure_5_6000.png"></p>
</li>
</ul>
</li>
</ul>
<h4 id="%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C">最终结果</h4>
<ul>
<li><strong>可以看到，在训练集数在4000-6000的范围里，平均奖励在300左右，峰值可以达到环境上界</strong></li>
<li><strong>在4000次训练中，结果比较集中在[350,500]的区间，但仍有部分样本结果在200以下</strong></li>
<li><strong>在5000次训练中，结果比较集中在[320,400]的区间内，体现了sarsa算法比较稳定的特点，但是没有能达到峰值的结果</strong></li>
<li><strong>在6000次训练中，结果比较集中在[300,500]的区间内，无论是稳定性还是平均结果都没有4000次和5000次优秀，说明可能Q表在训练到4000到
5000次时已达到收敛，但不一定能收敛到最佳的结果</strong></li>
</ul>
<h3 id="4-q%E5%AD%A6%E4%B9%A0q-learning">4. Q学习(Q-Learning)</h3>
<h4 id="%E5%8E%9F%E7%90%86">原理</h4>
<p>Q学习是一种异策略（off-policy）算法。异策略在学习的过程中，有两种不同的策略：目标策略（target policy）和行为策略（behavior policy）。 目标策略是我们需要去学习的策略，一般用 ${π}$ 来表示。
目标策略就像是在后方指挥战术的一个军师，它可以根据自己的经验来学习最优的策略，不需要去和环境交互。 行为策略是探索环境的策略，一般用
${μ}$ 来表示。行为策略可以大胆地去探索到所有可能的轨迹，采集轨迹，采集数据，然后把采集到的数据“喂”给目标策略学习。
在异策略学习的过程中，轨迹都是行为策略与环境交互产生的，产生这些轨迹后，我们使用这些轨迹来更新目标策略${π}$。</p>
<h4 id="%E5%AE%9E%E6%96%BD%E6%AD%A5%E9%AA%A4">实施步骤</h4>
<ul>
<li>
<p>初始化:</p>
<ul>
<li>Q表格：初始化Q表格，其中包含所有状态-动作对的Q值（通常设为0）。</li>
<li>超参数：学习率${α}$（通常0到1之间）、折扣因子${γ}$（通常0到1之间）、探索策略（如${ε}$-贪心策略）。</li>
</ul>
</li>
<li>
<p>策略学习过程:</p>
<ul>
<li>循环:
<ul>
<li>初始化环境并获取初始状态。</li>
<li>根据策略（例如${ε}$-贪心）选择动作a。</li>
<li>执行动作a，获得奖励r，并进入下一个状态s'。</li>
<li>更新Q值： ${ Q(s, a) = Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] }$</li>
<li>更新当前状态为下一个状态s'。</li>
<li>如果达到终端状态，结束当前回合。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>收敛与策略提取:</p>
<ul>
<li>随着迭代的进行，Q表中的值会逐渐逼近于最优Q值。</li>
<li>每个状态的最优动作可以通过： ${ \pi(s) = \arg\max_{a} Q(s, a) }$
${[ \sum_{t=0}^{\infty} \alpha_t = \infty \quad \text{and} \quad \sum_{t=0}^{\infty} \alpha_t^2 &lt; \infty ]}$</li>
</ul>
</li>
</ul>
<h4 id="%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</h4>
<ul>
<li>离散化：跟上一个算法一样，这里略去。</li>
<li>动作选择：仍采用${/epsilon}$-greedy策略，与上一个算法一样，这里略去。</li>
<li>Q-learning学习过程：<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_q_learning</span><span class="hljs-params">(env, n_episodes=<span class="hljs-number">1000</span>, alpha=<span class="hljs-number">0.1</span>, gamma=<span class="hljs-number">0.99</span>, epsilon=<span class="hljs-number">0.1</span>, n_bins=<span class="hljs-params">(<span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>)</span>)</span>:</span>
  Q = defaultdict(<span class="hljs-keyword">lambda</span>: np.zeros(env.action_space.n))
  reward_episodes = []
  loss_episodes = []
  <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> range(n_episodes):
      state = env.reset()
      state = discretize_state(state, n_bins)
      total_reward = <span class="hljs-number">0</span>
      done = <span class="hljs-literal">False</span>
      losses = []
      <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
          <span class="hljs-comment"># epsilon-greedy 策略选择动作</span>
          action = choose_action(state, Q, epsilon, env.action_space.n)

          <span class="hljs-comment"># 执行动作</span>
          next_state, reward, done, _ = env.step(action)
          next_state = discretize_state(next_state, n_bins)
          total_reward += reward

          <span class="hljs-comment"># 更新 Q 值</span>
          <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> done:
              best_next_action = np.argmax(Q[next_state])
              td_target = reward + gamma * Q[next_state][best_next_action]
          <span class="hljs-keyword">else</span>:
              td_target = reward
          td_error = td_target - Q[state][action]
          losses.append(alpha*td_error)
          Q[state][action] += alpha * td_error

          <span class="hljs-comment"># 移动到下一个状态</span>
          state = next_state
      reward_episodes.append(total_reward)
      loss_episodes.append(max(losses))

      <span class="hljs-keyword">if</span> episode % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
          print(<span class="hljs-string">f"Episode <span class="hljs-subst">{episode}</span>, Total Reward: <span class="hljs-subst">{total_reward}</span>"</span>)
  plt.plot(reward_episodes)
  plt.xlabel(<span class="hljs-string">'Episode'</span>)
  plt.ylabel(<span class="hljs-string">'Reward'</span>)
  plt.title(<span class="hljs-string">'Q-learning Training'</span>)
  <span class="hljs-comment"># plt.savefig('../Q-learning_results/Q-learning Training.png')</span>
  plt.show()

  <span class="hljs-keyword">return</span> Q
</div></code></pre>
</li>
</ul>
<h4 id="%E8%AE%AD%E7%BB%83%E9%9B%86%E6%95%B0%E8%B0%83%E6%95%B4">训练集数调整</h4>
<ul>
<li>
<p>参数设置为，</p>
<pre class="hljs"><code><div>n_episodes = <span class="hljs-number">5000</span> <span class="hljs-comment"># 训练次数</span>
alpha = <span class="hljs-number">0.1</span>  <span class="hljs-comment"># 学习率</span>
gamma = <span class="hljs-number">0.99</span> <span class="hljs-comment"># 折扣因子</span>
epsilon = <span class="hljs-number">0.1</span> <span class="hljs-comment"># epsilon-greedy 策略参数</span>
num_bin = <span class="hljs-number">6</span>  <span class="hljs-comment"># 状态空间离散化的分箱数</span>
n_bins = (num_bin, num_bin, num_bin, num_bin)
</div></code></pre>
</li>
<li>
<p>为探究训练集数对实验结果的影响，我们选取了4000，5000两个不同的训练集数进行实验。并将训练得到的策略和Q表格进行100次测试，并计算平均奖励。</p>
<p><strong>episode=4000：</strong></p>
<p><img src="Q-learning_results/Figure_1.png" alt="alt text">
<img src="Q-learning_results/Figure_2.png" alt="Figure_2.png"></p>
<p><strong>episode=5000：</strong></p>
<p><img src="Q-learning_results/training_5000.png" alt="training_5000.png">
<img src="Q-learning_results/test_5000.png" alt="test_5000.png"></p>
</li>
</ul>
<h4 id="%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C">最终结果</h4>
<ul>
<li><strong>可以看到，在训练集数在4000-5000的范围里，平均奖励在200左右，峰值可以达到环境上界</strong></li>
<li><strong>4000或5000次训练，基本都以较快的速度上升，在1000-2000集时就可以达到峰值</strong></li>
<li><strong>Q-learning收敛较快，但不如sarsa稳定，测试结果均方差较大</strong></li>
<li><strong>Q-learning在训练集数较少时，效果较好，但在训练集数较多时，效果反而不如sarsa</strong></li>
</ul>
<h3 id="5-%E6%80%BB%E7%BB%93">5. 总结</h3>
<h4 id="1mcdptd%E7%9A%84%E5%8C%BA%E5%88%AB">1.MC，DP，TD的区别</h4>
<p>自举是指更新时使用了估计。蒙特卡洛方法没有使用自举，因为它根据实际的回报进行更新。 动态规划方法和时序差分方法使用了自举。</p>
<p>采样是指更新时通过采样得到一个期望。 蒙特卡洛方法是纯采样的方法。 动态规划方法没有使用采样，它是直接用贝尔曼期望方程来更新状态价值的。 时序差分方法使用了采样。时序差分目标由两部分组成，一部分是采样，一部分是自举。</p>
<p>如果时序差分方法需要更广度的更新，就变成了 动态规划方法（因为动态规划方法是把所有状态都考虑进去来进行更新）。如果时序差分方法需要更深度的更新，就变成了蒙特卡洛方法。</p>
<img alt="TD_MC_DP.png" height="500" src="image/TD_MC_DP.png" width="500"/>
<h4 id="2sarsa%E5%92%8Cq-learning%E7%9A%84%E5%8C%BA%E5%88%AB">2.SARSA和Q-learning的区别</h4>
<p>Sarsa 是一个典型的同策略算法，它只用了一个策略 π，它不仅使用策略 π 学习，还使用策略
π 与环境交互产生经验。 如果策略采用 ε-贪心算法，它需要兼顾探索，为了兼顾探索和利用，它训练的时候会显得有点“胆小”。它在解决悬崖行走问题的时候，会尽可能地远离悬崖边，确保哪怕自己不小心探索了一点儿，也还是在安全区域内。此外，因为采用的是
ε-贪心 算法，策略会不断改变（ ε 值会不断变小），所以策略不稳定。</p>
<p>Q学习是一个典型的异策略算法，它有两种策略————目标策略和行为策略，它分离了目标策略与行为策略。Q学习可以大胆地用行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。行为策略可以采用 ε-贪心 算法，但目标策略采用的是贪心算法，它直接根据行为策略采集到的数据来采用最佳策略，所以 Q学习 不需要兼顾探索。</p>
<p>我们比较一下 Q学习 和 Sarsa 的更新公式，就可以发现 Sarsa 并没有选取最大值的最大化操作。因此，Q学习是一个非常激进的方法，它希望每一步都获得最大的利益；Sarsa 则相对较为保守，它会选择一条相对安全的迭代路线。</p>
<h2 id="%E4%BA%8C%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E6%96%B9%E6%B3%95">二、策略梯度方法</h2>
<h3 id="1reinforce%E7%AE%97%E6%B3%95">1.REINFORCE算法</h3>
<h4 id="%E5%8E%9F%E7%90%86">原理</h4>
<p>REINFORCE是一种基于策略的强化学习算法，属于策略梯度方法。其主要思想是直接优化策略，使得累积的期望回报最大化。REINFORCE算法通过更新参数化策略来学习智能体的行为策略。以下是REINFORCE算法的基本原理：</p>
<ul>
<li>
<p>策略参数化：假设策略是参数化的，记作 ${\pi(a|s; \theta)}$，这里 ${\pi}$ 是策略函数，${(a|s)}$ 是在状态 ${s}$ 下采取动作 ${a}$ 的概率，${\theta}$ 是策略的参数。</p>
</li>
<li>
<p>采样和回报：智能体根据当前策略与环境交互，采样出一个状态-动作序列，同时计算每个时间步之后的累计回报 ${G_t}$。</p>
</li>
<li>
<p>策略梯度：计算梯度 ${\nabla_\theta J(\theta)}$，以优化策略 ${\pi(a|s; \theta)}$ 以最大化期望回报。对于一个完整的轨迹 ${\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \ldots )}$，梯度估计为：</p>
<p>$${ \nabla_\theta J(\theta) = \mathbb{E}\tau [ \sum{t=0}^T \nabla_\theta \log \pi(a_t|s_t; \theta) G_t ] }$$</p>
</li>
<li>
<p>参数更新：通过梯度上升或下降，结合学习率 ${\alpha}$，逐步更新策略参数：</p>
</li>
</ul>
<p>$${ \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta) }$$</p>
<ul>
<li>
<p>去偏估计：由于回报 ${G_t}$ 是一个有偏估计，可以减去一个基线 ${b(s)}$ 来降低方差而不改变梯度的期望，通常选择状态值函数 ${V(s_t)}$ 作为基线：</p>
<p>$${ \nabla_\theta J(\theta) = \mathbb{E}\tau [ \sum{t=0}^T \nabla_\theta \log \pi(a_t|s_t; \theta) (G_t - b(s_t)) ] }$$</p>
</li>
</ul>
<p>REINFORCE算法的优势在于其简单性和理论上可以收敛到全局最优策略的可能性，但由于回报的高方差，学习稳定性和效率可能较差，尤其是在复杂或连续状态空间的环境中。</p>
<h4 id="%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</h4>
<ul>
<li>
<p>定义策略网络：定义一个简单的前馈神经网络作为策略网络，输入为状态，输出为动作的概率分布。分别用一层线性层作为输入层，ReLU激活函数，和输出层的Softmax激活函数。</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PolicyNetwork</span><span class="hljs-params">(nn.Module)</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_space, action_space)</span>:</span>
      super(PolicyNetwork, self).__init__()
      self.fc = nn.Sequential(
          nn.Linear(state_space, <span class="hljs-number">128</span>),
          nn.ReLU(), <span class="hljs-comment"># 使用ReLU激活函数，增加网络的非线性</span>
          nn.Linear(<span class="hljs-number">128</span>, action_space),
          nn.Softmax(dim=<span class="hljs-number">-1</span>)
      )

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
      <span class="hljs-keyword">return</span> self.fc(x)
</div></code></pre>
</li>
<li>
<p>reinforce算法：迭代每个episode：重置环境以开始新的episode，初始化存储对数概率和奖励的列表。在一个episode内，不断采样动作，直至完成：
state转换为张量并增加维度。
policy_net(state)返回动作概率。
Categorical(probs)定义分类分布用于采样动作。
action是从该分布采样的动作。
log_prob是所选动作的对数概率。
在每个回合进行采样后，按照时间顺序逆序计算累积回报，并标准化。
再计算损失，将log_prob与标准化的回报相乘，然后取负值。
最后对损失进行反向传播，更新策略网络的参数。</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">from</span> torch.distributions <span class="hljs-keyword">import</span> Categorical
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reinforce</span><span class="hljs-params">(env, policy_net, optimizer, num_episodes=<span class="hljs-number">1000</span>, gamma=<span class="hljs-number">0.99</span>)</span>:</span>
  reward_episode = []
  <span class="hljs-keyword">for</span> episode <span class="hljs-keyword">in</span> range(num_episodes):
      state = env.reset()
      log_probs = []
      rewards = []

      <span class="hljs-comment"># 收集一条轨迹</span>
      done = <span class="hljs-literal">False</span>
      <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
          state = torch.FloatTensor(state).unsqueeze(<span class="hljs-number">0</span>)
          probs = policy_net(state) <span class="hljs-comment"># 获取动作概率</span>
          m = Categorical(probs)
          action = m.sample()
          log_prob = m.log_prob(action)

          new_state, reward, done, _ = env.step(action.item())

          log_probs.append(log_prob)
          rewards.append(reward)
          state = new_state

      <span class="hljs-comment"># 计算返回（回报）和更新策略</span>
      returns = []
      G = <span class="hljs-number">0</span>
      <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> reversed(rewards):
          G = r + gamma * G
          returns.insert(<span class="hljs-number">0</span>, G)
      returns = torch.tensor(returns)

      <span class="hljs-comment"># 标准化回报</span>
      returns = (returns - returns.mean()) / (returns.std() + <span class="hljs-number">1e-6</span>)

      loss = []
      <span class="hljs-keyword">for</span> log_prob, G <span class="hljs-keyword">in</span> zip(log_probs, returns):
          loss.append(-log_prob * G)

      optimizer.zero_grad()
      loss = torch.cat(loss).sum()  <span class="hljs-comment"># 将loss列表中的tensor拼接成一个tensor，然后求和</span>
      loss.backward() <span class="hljs-comment"># 反向传播</span>
      optimizer.step() <span class="hljs-comment"># 更新参数</span>

      reward_episode.append(sum(rewards))

      <span class="hljs-comment"># 打印进度</span>
      <span class="hljs-keyword">if</span> episode % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
          print(<span class="hljs-string">f'Episode <span class="hljs-subst">{episode}</span>/<span class="hljs-subst">{num_episodes}</span>, Total Reward: <span class="hljs-subst">{sum(rewards)}</span>'</span>)

  print(<span class="hljs-string">'Training Complete!'</span>)
  plt.plot(reward_episode)
  plt.xlabel(<span class="hljs-string">'Episode'</span>)
  plt.ylabel(<span class="hljs-string">'Total Reward'</span>)
  plt.title(<span class="hljs-string">'REINFORCE Training'</span>)
  plt.show()
  torch.save(policy_net.state_dict(), <span class="hljs-string">'../Policy/PG_policy_net.pth'</span>)
</div></code></pre>
<p>最后，我们训练策略网络并保存模型参数。在训练过程中，我们可以看到每个episode的总奖励，以及训练完成后的总奖励曲线。</p>
</li>
<li>
<p>训练过程：</p>
<pre class="hljs"><code><div><span class="hljs-comment"># 初始化环境</span>
env = gym.make(<span class="hljs-string">'CartPole-v1'</span>)
state_space = env.observation_space.shape[<span class="hljs-number">0</span>]
action_space = env.action_space.n

<span class="hljs-comment"># 创建策略网络和优化器</span>
policy_net = PolicyNetwork(state_space, action_space)
optimizer = optim.Adam(policy_net.parameters(), lr=<span class="hljs-number">1e-2</span>)

<span class="hljs-comment"># 运行REINFORCE算法</span>
reinforce(env, policy_net, optimizer)

<span class="hljs-comment"># 测试策略网络</span>
test_policy(env, policy_net)
env.close()
</div></code></pre>
</li>
</ul>
<h4 id="%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C">训练结果</h4>
<ul>
<li>
<p>训练结果如下：</p>
<p><img src="PG_results/Training.png" alt="alt text"></p>
<p><strong>可以看到，在训练100次左右首次达到峰值，最后在大约500次后收敛到500。</strong></p>
</li>
<li>
<p>测试结果如下：</p>
<p><img src="PG_results/Test.png" alt="alt text"></p>
<p><strong>可以看到，测试结果全部500，表现优秀。</strong></p>
</li>
</ul>
<h4 id="%E6%80%BB%E7%BB%93">总结</h4>
<ul>
<li>REINFORCE算法是一种基于策略的强化学习算法，通过直接优化策略来最大化累积的期望回报。</li>
<li>REINFORCE算法的优势在于其简单性和理论上可以收敛到全局最优策略的可能性，但由于回报的高方差，学习稳定性和效率可能较差，尤其是在复杂或连续状态空间的环境中。</li>
</ul>
<h3 id="2ppo%E7%AE%97%E6%B3%95">2.PPO算法</h3>
<h4 id="%E5%8E%9F%E7%90%86">原理</h4>
<p>近邻策略优化（Proximal Policy Optimization, PPO）是一种先进的强化学习算法，属于策略梯度方法。PPO方法引入了新的策略更新机制，用以平衡探索和收敛之间的矛盾，增强策略优化的稳定性和鲁棒性。其基本原理可以总结如下：</p>
<ul>
<li>
<p>策略限制：
PPO在策略更新时引入了限制，确保新旧策略之间的更新幅度不会太大。这个限制通过设计目标函数中的剪切（clipping）来控制偏差，以防止策略过快变化导致不稳定。</p>
</li>
<li>
<p>目标函数：
PPO使用的是一个被称为剪切策略目标函数（Clipped Surrogate Objective）的函数形式：</p>
<p>$${ L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right] }$$</p>
<p>其中，${ r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} }$，表示新旧策略概率之比，${\hat{A}_t}$是优势估计，${\epsilon}$是超参数用于限制更新范围。</p>
</li>
<li>
<p>优势函数：
优势函数 ${\hat{A}<em>t}$ 标识了某动作在当前策略下优于均值策略的程度。优势估计可以用值函数近似，常使用广义优势估计（GAE, Generalized Advantage Estimation）方法:
$${\hat{A}t = \delta_t + (\gamma \lambda) \delta{t+1} + (\gamma \lambda)^2 \delta</em>{t+2} + \cdots }$$</p>
<p>其中，${\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)}$ 是TD误差。</p>
</li>
<li>
<p>神经网络参数化：
PPO通常使用两个神经网络，一个用于策略（Actor），另一个用于状态值函数（Critic）。这些网络通过交替优化更新。</p>
</li>
<li>
<p>交替更新：
PPO在同一批数据上进行多个epoch的策略更新，但每次更新都限制了策略的变化。这种方法能够在不增加环境交互采样次数的情况下，提高策略更新的样本效率。
易用性和鲁棒性：</p>
</li>
</ul>
<p>相较于其他复杂的策略优化方法，PPO简单、易于实现，具有良好的稳定性和性能，在处理大规模任务时表现出色。
PPO流行的原因在于它兼顾了算法表现和实现复杂度，在多个连续控制和组合优化任务中表现优良，其策略更新机制为各种应用提供更可靠的优化策略。</p>
<h4 id="%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</h4>
<ul>
<li>定义策略网络和价值网络，分别用于策略和状态值函数的参数化。<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PolicyNet</span><span class="hljs-params">(nn.Module)</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_dim, hidden_dim, action_dim)</span>:</span>
      super().__init__()
      self.fc1 = nn.Linear(state_dim, hidden_dim)
      self.fc2 = nn.Linear(hidden_dim, action_dim)

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
      x = F.relu(self.fc1(x))
      <span class="hljs-keyword">return</span> F.softmax(self.fc2(x), dim=<span class="hljs-number">1</span>)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ValueNet</span><span class="hljs-params">(nn.Module)</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_dim, hidden_dim)</span>:</span>
      super().__init__()
      self.fc1 = nn.Linear(state_dim, hidden_dim)
      self.fc2 = nn.Linear(hidden_dim, <span class="hljs-number">1</span>)

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
      x = F.relu(self.fc1(x))
      <span class="hljs-keyword">return</span> self.fc2(x)
</div></code></pre>
</li>
<li>PPO算法：定义PPO算法，包括策略更新、价值函数更新、优势估计、目标函数等。<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PPO</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, epochs, eps, gamma, device)</span>:</span>
      self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
      self.critic = ValueNet(state_dim, hidden_dim).to(device)
      self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
      self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
      self.gamma = gamma
      self.lmbda = lmbda
      self.epochs = epochs    
      self.eps = eps  
      self.device = device

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">take_action</span><span class="hljs-params">(self, state)</span>:</span> <span class="hljs-comment"># 选择动作</span>
      state = torch.FloatTensor([state]).to(self.device)
      probs = self.actor(state)
      action_dist = torch.distributions.Categorical(probs) <span class="hljs-comment"># 定义分类分布</span>
      action = action_dist.sample() <span class="hljs-comment"># 采样动作</span>
      <span class="hljs-keyword">return</span> action.item()

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gae</span><span class="hljs-params">(self, td_delta)</span>:</span> <span class="hljs-comment"># 计算广义优势估计</span>
      td_delta = td_delta.detach().numpy() <span class="hljs-comment"># 转换为numpy数组</span>
      advantages_list = [] <span class="hljs-comment"># 优势列表</span>
      advantage = <span class="hljs-number">0.0</span> <span class="hljs-comment"># 优势</span>
      <span class="hljs-keyword">for</span> delta <span class="hljs-keyword">in</span> td_delta[::<span class="hljs-number">-1</span>]:
          advantage = self.gamma * self.lmbda * advantage + delta <span class="hljs-comment"># 计算优势</span>
          advantages_list.append(advantage)
      advantages_list.reverse() <span class="hljs-comment"># 反转列表</span>
      <span class="hljs-keyword">return</span> torch.FloatTensor(advantages_list)

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(self, transition_dist)</span>:</span> <span class="hljs-comment"># 更新策略和价值网络</span>
      states = torch.FloatTensor(transition_dist[<span class="hljs-string">'states'</span>]).to(self.device)
      actions = torch.tensor(transition_dist[<span class="hljs-string">'actions'</span>]).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)).to(self.device)
      rewards = torch.FloatTensor(transition_dist[<span class="hljs-string">'rewards'</span>]).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)).to(self.device)
      next_states = torch.FloatTensor(transition_dist[<span class="hljs-string">'next_states'</span>]).to(self.device)
      dones = torch.FloatTensor(transition_dist[<span class="hljs-string">'dones'</span>]).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)).to(self.device)
      td_target = rewards + self.gamma * self.critic(next_states) * (<span class="hljs-number">1</span> - dones) <span class="hljs-comment"># 计算TD目标</span>
      td_delta = td_target - self.critic(states) <span class="hljs-comment"># 计算TD误差</span>
      <span class="hljs-comment"># GAE 计算广义优势</span>
      advantage = self.gae(td_delta.cpu()).to(self.device) 
      old_log_probs = torch.log(self.actor(states).gather(<span class="hljs-number">1</span>, actions)).detach() <span class="hljs-comment"># 旧的对数概率</span>

      <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(self.epochs):
          log_probs = torch.log(self.actor(states).gather(<span class="hljs-number">1</span>, actions)) <span class="hljs-comment"># 对数概率</span>
          ration = torch.exp(log_probs - old_log_probs) <span class="hljs-comment"># 比率</span>
          surr1 = ration * advantage <span class="hljs-comment"># 损失1</span>
          surr2 = torch.clamp(ration, <span class="hljs-number">1</span>-self.eps, <span class="hljs-number">1</span>+self.eps) * advantage <span class="hljs-comment"># 截断</span>
          actor_loss = torch.mean(-torch.min(surr1, surr2))   <span class="hljs-comment"># PPO损失函数</span>
          critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))
          self.actor_optimizer.zero_grad()
          self.critic_optimizer.zero_grad()
          actor_loss.backward()
          critic_loss.backward()
          self.actor_optimizer.step()
          self.critic_optimizer.step()
</div></code></pre>
</li>
<li>训练过程：<pre class="hljs"><code><div><span class="hljs-comment"># 初始化环境</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>
  actor_lr = <span class="hljs-number">1e-3</span>
  critic_lr = <span class="hljs-number">1e-2</span>
  num_episodes = <span class="hljs-number">500</span>
  hidden_dim = <span class="hljs-number">128</span>
  gamma = <span class="hljs-number">0.98</span>
  lmbda = <span class="hljs-number">0.95</span>
  epochs = <span class="hljs-number">10</span>
  eps = <span class="hljs-number">0.2</span>
  device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)

  env_name = <span class="hljs-string">"CartPole-v1"</span>
  env = gym.make(env_name)
  torch.manual_seed(<span class="hljs-number">0</span>)
  state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]
  action_dim = env.action_space.n
  agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, epochs, eps, gamma, device)

  return_list = []
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):
      <span class="hljs-keyword">with</span> tqdm(total=int(num_episodes / <span class="hljs-number">10</span>), desc=<span class="hljs-string">'Iteration %d'</span> % i) <span class="hljs-keyword">as</span> pbar:
          <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> range(int(num_episodes / <span class="hljs-number">10</span>)):
              episode_return = <span class="hljs-number">0</span>
              transition_dict = {<span class="hljs-string">'states'</span>: [], <span class="hljs-string">'actions'</span>: [], <span class="hljs-string">'next_states'</span>: [], <span class="hljs-string">'rewards'</span>: [], <span class="hljs-string">'dones'</span>: []}
              state= env.reset()
              done = <span class="hljs-literal">False</span>
              <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
                  action = agent.take_action(state)
                  next_state, reward, done,_= env.step(action)
                  <span class="hljs-comment"># print(next_state, reward, done,truncated)</span>
                  done = done    <span class="hljs-comment"># 这个地方要注意</span>
                  transition_dict[<span class="hljs-string">'states'</span>].append(state)
                  transition_dict[<span class="hljs-string">'actions'</span>].append(action)
                  transition_dict[<span class="hljs-string">'next_states'</span>].append(next_state)
                  transition_dict[<span class="hljs-string">'rewards'</span>].append(reward)
                  transition_dict[<span class="hljs-string">'dones'</span>].append(done)
                  state = next_state
                  episode_return += reward
              return_list.append(episode_return)
              agent.update(transition_dict)
              <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
                  pbar.set_postfix({<span class="hljs-string">'episode'</span>: <span class="hljs-string">'%d'</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),
                                    <span class="hljs-string">'return'</span>: <span class="hljs-string">'%.3f'</span> % np.mean(return_list[<span class="hljs-number">-10</span>:])})
              pbar.update(<span class="hljs-number">1</span>)
</div></code></pre>
</li>
</ul>
<h4 id="%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C">训练结果</h4>
<ul>
<li>
<p>训练结果如下：</p>
<p><img src="PPO_results/ppoTraining.png" alt="alt text"></p>
<p><strong>可以看到，在训练50次左右首次达到峰值，最后在大约500次后收敛到500。</strong></p>
</li>
<li>
<p>测试结果如下：</p>
<p><img src="PPO_results/ppoTest.png" alt="alt text"></p>
<p><strong>可以看到，测试结果全部500，表现优秀。</strong></p>
</li>
</ul>
<h4 id="%E6%80%BB%E7%BB%93">总结</h4>
<ul>
<li>PPO算法是一种先进的策略梯度方法，通过引入策略限制和优势估计，提高了策略优化的稳定性和鲁棒性。</li>
<li>PPO算法在处理大规模任务时表现出色，具有良好的稳定性和性能，易于实现和调整，适用于各种连续控制和组合优化任务。</li>
</ul>
<h2 id="%E4%B8%89%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95">三、深度学习方法</h2>
<h3 id="1dqn%E7%AE%97%E6%B3%95">1.DQN算法</h3>
<h4 id="%E5%8E%9F%E7%90%86">原理</h4>
<p>深度Q网络（Deep Q-Network, DQN）是结合经典Q学习和深度学习的强化学习算法，由DeepMind团队在2013年首次提出。DQN成功应用于复杂的视觉输入任务，并在很多情况下表现优异。以下是DQN的基本原理：</p>
<p>DQN算法基本原理</p>
<p>Q学习回顾：
Q学习是一种值迭代算法，它迭代更新 Q 值函数来估计在给定状态-动作对上的最优动作值。Q 值更新公式为：</p>
<p>$${ Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right] }$$
重新定义动作的价值并更新Q表，直到收敛于真实的Q值。</p>
<p>引入神经网络逼近 Q 函数：
在DQN中，一个深度神经网络用于表示和逼近 Q 值函数。输入是状态，输出是对应动作的 Q 值。</p>
<p>经验回放 (Experience Replay)： 环境交互经验不再直接用于训练，而是存储在记忆库中。训练时随机采样小批量经验，这种方式解决了数据相关性和非平稳分布的问题，提高样本效率。</p>
<p>目标网络 (Target Network)： 为了避免网络更新不稳定的问题，DQN使用两个网络：
当前网络（用于选择动作并更新）
目标网络（用于计算目标Q值 ${y_t = r + \gamma \max_{a'} Q_{\text{target}}(s', a')}$）
目标网络在达到一定步数后从当前网络复制参数，以减缓目标值变化。</p>
<p>损失函数： 使用均方误差 (MSE) 损失来最小化误差：
$${ L(\theta) = \mathbb{E}<em>{(s,a,r,s') \sim \mathcal{D}} \left[ \left( y_t - Q(s, a; \theta) \right)^2 \right] }$$
其中 ${y_t = r + \gamma \max</em>{a'} Q_{\text{target}}(s', a'; \theta^-) }$。</p>
<p>DQN的关键步骤</p>
<p>初始化：
初始化策略网络和目标网络参数。
初始化经验回放内存。</p>
<p>交互环境：
使用当前策略与环境交互，存储(state, action, reward, next_state, done)样本到记忆库。</p>
<p>从记忆库中采样经验：
随机抽取一个小批量样本用于训练。</p>
<p>计算目标 Q 值：
使用目标网络计算下一状态的最大 Q 值并更新目标值 ${y_t}$。</p>
<p>最小化损失：
使用反向传播算法更新策略网络参数。</p>
<p>更新目标网络：
以固定步数或者软更新策略更新目标网络参数。</p>
<p>DQN克服了传统Q学习在大型状态空间中的局限性，通过深度神经网络可以很好地处理高维输入并近似动作价值，从而被广泛用于复杂环境和任务中。</p>
<h4 id="%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</h4>
<ul>
<li>
<p>定义Q网络，包括初始化网络结构和前向传播过程。</p>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QNetwork</span><span class="hljs-params">(nn.Module)</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_dim, hidden_dim, action_dim)</span>:</span>
      super(QNetwork, self).__init__()
      self.fc1 = nn.Linear(state_dim, hidden_dim)
      self.fc2 = nn.Linear(hidden_dim, action_dim)

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
      x = F.relu(self.fc1(x))
      <span class="hljs-keyword">return</span> self.fc2(x)
</div></code></pre>
</li>
<li>
<p>定义DQN算法，包括初始化网络、目标网络、优化器、经验回放等。</p>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DQN</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_dim, hidden_dim, action_dim, lr, gamma, eps_start, eps_end, eps_decay, target_update,
               memory_capacity, batch_size, device)</span>:</span>
      self.q_network = QNetwork(state_dim, hidden_dim, action_dim).to(device)
      self.target_network = QNetwork(state_dim, hidden_dim, action_dim).to(device)
      self.target_network.load_state_dict(self.q_network.state_dict())
      self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=lr)
      self.gamma = gamma
      self.eps_start = eps_start
      self.eps_end = eps_end
      self.eps_decay = eps_decay
      self.epsilon = self.eps_start
      self.target_update = target_update
      self.memory = deque(maxlen=memory_capacity)
      self.batch_size = batch_size
      self.device = device
      self.action_dim = action_dim

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">select_action</span><span class="hljs-params">(self, state)</span>:</span>
      <span class="hljs-keyword">if</span> random.random() &gt; self.epsilon:
          <span class="hljs-keyword">with</span> torch.no_grad():
              state = torch.FloatTensor(state).to(self.device).unsqueeze(<span class="hljs-number">0</span>)
              q_values = self.q_network(state)
              <span class="hljs-keyword">return</span> q_values.max(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].item()
      <span class="hljs-keyword">else</span>:
          <span class="hljs-keyword">return</span> random.randrange(self.action_dim)

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">store_transition</span><span class="hljs-params">(self, state, action, reward, next_state, done)</span>:</span>
      self.memory.append((state, action, reward, next_state, done))

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(self, step)</span>:</span>
      <span class="hljs-keyword">if</span> len(self.memory) &lt; self.batch_size:
          <span class="hljs-keyword">return</span>

      batch = random.sample(self.memory, self.batch_size)
      states, actions, rewards, next_states, dones = zip(*batch)
      states = torch.FloatTensor(states).to(self.device)
      actions = torch.LongTensor(actions).unsqueeze(<span class="hljs-number">1</span>).to(self.device)
      rewards = torch.FloatTensor(rewards).unsqueeze(<span class="hljs-number">1</span>).to(self.device)
      next_states = torch.FloatTensor(next_states).to(self.device)
      dones = torch.FloatTensor(dones).unsqueeze(<span class="hljs-number">1</span>).to(self.device)

      curr_q_values = self.q_network(states).gather(<span class="hljs-number">1</span>, actions)
      next_q_values = self.target_network(next_states).max(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].detach().unsqueeze(<span class="hljs-number">1</span>)
      expected_q_values = rewards + self.gamma * next_q_values * (<span class="hljs-number">1</span> - dones)

      loss = F.mse_loss(curr_q_values, expected_q_values)

      self.optimizer.zero_grad()
      loss.backward()
      self.optimizer.step()

      self.epsilon = max(self.eps_end, self.eps_start - step / self.eps_decay)

      <span class="hljs-keyword">if</span> step % self.target_update == <span class="hljs-number">0</span>:
          self.target_network.load_state_dict(self.q_network.state_dict())
</div></code></pre>
</li>
<li>
<p>训练过程：</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>
    lr = <span class="hljs-number">1e-3</span>
    num_episodes = <span class="hljs-number">250</span>
    hidden_dim = <span class="hljs-number">128</span>
    gamma = <span class="hljs-number">0.98</span>
    eps_start = <span class="hljs-number">0.9</span>
    eps_end = <span class="hljs-number">0.05</span>
    eps_decay = <span class="hljs-number">500</span>
    target_update = <span class="hljs-number">10</span>
    memory_capacity = <span class="hljs-number">10000</span>
    batch_size = <span class="hljs-number">64</span>
    device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)

    env_name = <span class="hljs-string">"CartPole-v1"</span>
    env = gym.make(env_name)
    torch.manual_seed(<span class="hljs-number">0</span>)
    state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]
    action_dim = env.action_space.n

    agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, eps_start, eps_end, eps_decay, target_update,
                memory_capacity, batch_size, device)

    return_list = []
    step = <span class="hljs-number">0</span>

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):
        <span class="hljs-keyword">with</span> tqdm(total=int(num_episodes / <span class="hljs-number">10</span>), desc=<span class="hljs-string">'Iteration %d'</span> % i) <span class="hljs-keyword">as</span> pbar:
            <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> range(int(num_episodes / <span class="hljs-number">10</span>)):
                episode_return = <span class="hljs-number">0</span>
                state = env.reset()
                done = <span class="hljs-literal">False</span>
                <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
                    action = agent.select_action(state)
                    next_state, reward, done, _ = env.step(action)
                    agent.store_transition(state, action, reward, next_state, done)
                    agent.update(step)
                    step += <span class="hljs-number">1</span>
                    state = next_state
                    episode_return += reward
                return_list.append(episode_return)

                <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
                    pbar.set_postfix({<span class="hljs-string">'episode'</span>: <span class="hljs-string">'%d'</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),
                                      <span class="hljs-string">'return'</span>: <span class="hljs-string">'%.3f'</span> % np.mean(return_list[<span class="hljs-number">-10</span>:])})
                    <span class="hljs-keyword">if</span> np.mean(return_list[<span class="hljs-number">-10</span>:]) &gt; <span class="hljs-number">490</span>:
                        torch.save(agent.q_network.state_dict(), <span class="hljs-string">'../Policy/DQN_net_peak.pth'</span>)
                pbar.update(<span class="hljs-number">1</span>)
</div></code></pre>
</li>
<li>
<p>测试过程：</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>
    env_name = <span class="hljs-string">"CartPole-v1"</span>
    env = gym.make(env_name)
    state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]
    action_dim = env.action_space.n
    hidden_dim = <span class="hljs-number">128</span>
    device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)

    agent = DQN(state_dim, hidden_dim, action_dim, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, device)
    agent.q_network.load_state_dict(torch.load(<span class="hljs-string">'../Policy/DQN_net.pth'</span>))
    agent.q_network.eval()

    num_episodes = <span class="hljs-number">100</span>
    return_list = []

    <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> range(num_episodes):
        episode_return = <span class="hljs-number">0</span>
        state = env.reset()
        done = <span class="hljs-literal">False</span>
        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
            action = agent.select_action(state)
            next_state, reward, done, _ = env.step(action)
            state = next_state
            episode_return += reward
        return_list.append(episode_return)
        print(<span class="hljs-string">f'Episode <span class="hljs-subst">{i_episode}</span>/<span class="hljs-subst">{num_episodes}</span>, Total Reward: <span class="hljs-subst">{episode_return}</span>'</span>) <span class="hljs-keyword">if</span> i_episode % <span class="hljs-number">10</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>

    episodes_list = list(range(len(return_list)))
    env.close()

</div></code></pre>
</li>
</ul>
<h4 id="%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C">训练结果</h4>
<ul>
<li>
<p>训练结果如下：</p>
<p><img src="DQN_results/DQN_Training.png" alt="alt text"></p>
<p><strong>可以看到，在训练100次左右首次达到峰值，但是并没有始终没有达到完美收敛。</strong></p>
</li>
<li>
<p>测试结果如下：</p>
<p><img src="DQN_results/DQN_Testing.png" alt="alt text"></p>
<p><strong>可以看到，测试结果全部500，表现优秀。</strong></p>
</li>
</ul>
<h4 id="%E6%80%BB%E7%BB%93">总结</h4>
<ul>
<li>DQN在训练过程中稳定性表现不如PPO和REINFORCE，QN的训练稳定性依赖于几个因素包括经验回放的效果和目标网络的更新。可能会受到训练不稳定的影响，尤其是在处理高维或者连续状态空间时。</li>
<li>DQN本质上是服务于离散动作空间的，因此在连续动作空间上的表现不如PPO和REINFORCE。</li>
<li>DQN在收敛过程中依赖于经验回放和目标网络，因此需要更多的训练时间和样本。</li>
</ul>
<h3 id="2a2c%E7%AE%97%E6%B3%95">2.A2C算法</h3>
<h4 id="%E5%8E%9F%E7%90%86">原理</h4>
<p>A2C（Advantage Actor-Critic）是改进的Actor-Critic算法的一种同步版本，主要用于解决策略优化问题。A2C的基本思路是利用“优势函数”（Advantage Function）来减少策略梯度的方差，并通过同步多个环境来提高优化效率。这里是A2C算法的基本原理和实现步骤：</p>
<p>基本原理：</p>
<p>Actor-Critic架构：
Actor：负责策略的更新。通过策略网络直接输出动作的概率分布，从而决定动作。
Critic：评估当前策略的好坏。通过价值网络（或Critic网络）输出状态值函数，用来指引Actor的学习。
优势函数（Advantage Function）：</p>
<p>优势函数用来衡量某个动作相对于当前策略下平均动作的好坏程度。定义为： ${ A(s, a) = Q(s, a) - V(s) }$
通过减少策略梯度的方差，提高学习效率。</p>
<p>异步 &amp; 同步机制： A2C是同步的版本，采用多个环境并行运行，从中收集样本，通过平均梯度的方式来更新网络参数，这样可以提高样本效率和收敛速度。</p>
<p>实现步骤：</p>
<p>初始化策略网络和价值网络：两个网络共用一些层（特征共享部分），输出动作分布和状态值。</p>
<p>同步环境：初始化多个环境实例以并行运行。</p>
<p>并行采样：
在多个环境中同步采集样本：状态、动作、奖励和下一状态。
采用固定步数或一个完整的episode结束后更新参数。</p>
<p>计算奖励折现和优势值：
使用折扣因子γ累积未来的奖励并计算优势值： ${ R_t = r_t + \gamma V(s_{t+1}) }$,${ A(s, a) = r + \gamma V(s_{t+1}) - V(s_t) }$
计算损失函数：
策略损失 (使用优势值)： ${ L_{policy} = -\log \pi(a|s) \cdot A(s, a) }$
价值损失： ${ L_{value} = (R - V(s))^2 }$
熵损失（用于改善探索）： ${ L_{entropy} = \sum \pi(a|s) \log \pi(a|s) }$
总损失计算： ${ L = L_{policy} + c_1 \cdot L_{value} - c_2 \cdot L_{entropy} }$</p>
<p>反向传播和更新网络参数：
计算梯度并更新策略网络和价值网络的参数。</p>
<p>重复：
返回步骤3，继续采样和更新网络，直到达到终止条件（例如，设置的最大迭代次数或达到满意的性能）。
通过这些步骤，A2C能够有效地学习策略，在不同的环境中表现出良好的性能。A2C特别适合需要快速收敛的任务，因为它能够利用并行性提高训练效率。</p>
<h4 id="%E6%BA%90%E4%BB%A3%E7%A0%81">源代码</h4>
<ul>
<li>
<p>定义策略网络和价值网络，包括初始化网络结构和前向传播过程。</p>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PolicyNet</span><span class="hljs-params">(nn.Module)</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_dim, hidden_dim, action_dim)</span>:</span>
      super().__init__()
      self.fc1 = nn.Linear(state_dim, hidden_dim)
      self.fc2 = nn.Linear(hidden_dim, action_dim)

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
      x = F.relu(self.fc1(x))
      <span class="hljs-keyword">return</span> F.softmax(self.fc2(x), dim=<span class="hljs-number">1</span>)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ValueNet</span><span class="hljs-params">(nn.Module)</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_dim, hidden_dim)</span>:</span>
      super().__init__()
      self.fc1 = nn.Linear(state_dim, hidden_dim)
      self.fc2 = nn.Linear(hidden_dim, <span class="hljs-number">1</span>)
      
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
      x = F.relu(self.fc1(x))
      <span class="hljs-keyword">return</span> self.fc2(x)
</div></code></pre>
</li>
<li>
<p>定义A2C算法，包括初始化网络、优化器、损失函数、采样和更新等。</p>
<pre class="hljs"><code><div><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">A2C</span>:</span>
  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, gamma, device)</span>:</span>
      self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)
      self.critic = ValueNet(state_dim, hidden_dim).to(device)
      self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
      self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)
      self.gamma = gamma
      self.device = device

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">take_action</span><span class="hljs-params">(self, state)</span>:</span>
      state = torch.FloatTensor([state]).to(self.device)
      probs = self.actor(state)
      action_dist = torch.distributions.Categorical(probs)
      action = action_dist.sample()
      <span class="hljs-keyword">return</span> action.item()

  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(self, transition_dict)</span>:</span>
      states = torch.FloatTensor(transition_dict[<span class="hljs-string">'states'</span>]).to(self.device)
      actions = torch.tensor(transition_dict[<span class="hljs-string">'actions'</span>]).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)).to(self.device)
      rewards = torch.FloatTensor(transition_dict[<span class="hljs-string">'rewards'</span>]).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)).to(self.device)
      next_states = torch.FloatTensor(transition_dict[<span class="hljs-string">'next_states'</span>]).to(self.device)
      dones = torch.FloatTensor(transition_dict[<span class="hljs-string">'dones'</span>]).reshape((<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>)).to(self.device)

      <span class="hljs-comment"># Compute TD target</span>
      td_target = rewards + self.gamma * self.critic(next_states) * (<span class="hljs-number">1</span> - dones)
      td_delta = td_target - self.critic(states)

      <span class="hljs-comment"># Actor Loss</span>
      log_probs = torch.log(self.actor(states).gather(<span class="hljs-number">1</span>, actions))
      actor_loss = -(log_probs * td_delta.detach()).mean()

      <span class="hljs-comment"># Critic Loss</span>
      critic_loss = F.mse_loss(self.critic(states), td_target.detach())

      <span class="hljs-comment"># Update Actor Network</span>
      self.actor_optimizer.zero_grad()
      actor_loss.backward()
      self.actor_optimizer.step()

      <span class="hljs-comment"># Update Critic Network</span>
      self.critic_optimizer.zero_grad()
      critic_loss.backward()
      self.critic_optimizer.step()
</div></code></pre>
</li>
<li>
<p>训练过程：</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">()</span>:</span>
  actor_lr = <span class="hljs-number">1e-3</span>
  critic_lr = <span class="hljs-number">1e-2</span>
  num_episodes = <span class="hljs-number">1000</span>
  hidden_dim = <span class="hljs-number">128</span>
  gamma = <span class="hljs-number">0.98</span>
  device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)
  env_name = <span class="hljs-string">"CartPole-v1"</span>
  env = gym.make(env_name)
  torch.manual_seed(<span class="hljs-number">0</span>)
  state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]
  action_dim = env.action_space.n

  agent = A2C(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, gamma, device)
  return_list = []

  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):
      <span class="hljs-keyword">with</span> tqdm(total=int(num_episodes / <span class="hljs-number">10</span>), desc=<span class="hljs-string">'Iteration %d'</span> % i) <span class="hljs-keyword">as</span> pbar:
          <span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> range(int(num_episodes / <span class="hljs-number">10</span>)):
              episode_return = <span class="hljs-number">0</span>
              transition_dict = {<span class="hljs-string">'states'</span>: [], <span class="hljs-string">'actions'</span>: [], <span class="hljs-string">'next_states'</span>: [], <span class="hljs-string">'rewards'</span>: [], <span class="hljs-string">'dones'</span>: []}
              state = env.reset()
              done = <span class="hljs-literal">False</span>
              <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
                  action = agent.take_action(state)
                  next_state, reward, done, _ = env.step(action)
                  transition_dict[<span class="hljs-string">'states'</span>].append(state)
                  transition_dict[<span class="hljs-string">'actions'</span>].append(action)
                  transition_dict[<span class="hljs-string">'next_states'</span>].append(next_state)
                  transition_dict[<span class="hljs-string">'rewards'</span>].append(reward)
                  transition_dict[<span class="hljs-string">'dones'</span>].append(done)
                  state = next_state
                  episode_return += reward
              return_list.append(episode_return)
              agent.update(transition_dict)
              <span class="hljs-keyword">if</span> (i_episode + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
                  pbar.set_postfix({<span class="hljs-string">'episode'</span>: <span class="hljs-string">'%d'</span> % (num_episodes / <span class="hljs-number">10</span> * i + i_episode + <span class="hljs-number">1</span>),
                                    <span class="hljs-string">'return'</span>: <span class="hljs-string">'%.3f'</span> % np.mean(return_list[<span class="hljs-number">-10</span>:])})
              pbar.update(<span class="hljs-number">1</span>)

  env.close()
</div></code></pre>
</li>
<li>
<p>测试过程：</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">()</span>:</span>
  num_episodes = <span class="hljs-number">100</span>
  env_name = <span class="hljs-string">"CartPole-v1"</span>
  env = gym.make(env_name)
  state_dim = env.observation_space.shape[<span class="hljs-number">0</span>]
  action_dim = env.action_space.n
  hidden_dim = <span class="hljs-number">128</span>
  device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)

  agent = A2C(state_dim, hidden_dim, action_dim, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, device)
  agent.actor.load_state_dict(torch.load(<span class="hljs-string">'../Policy/A2C_actor.pth'</span>))
  agent.critic.load_state_dict(torch.load(<span class="hljs-string">'../Policy/A2C_critic.pth'</span>))
  agent.actor.eval()
  agent.critic.eval()

  return_list = []
  <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_episodes):
      episode_return = <span class="hljs-number">0</span>
      state = env.reset()
      done = <span class="hljs-literal">False</span>
      <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
          action = agent.take_action(state)
          state, reward, done, _ = env.step(action)
          episode_return += reward
      return_list.append(episode_return)
      print(<span class="hljs-string">f'Episode <span class="hljs-subst">{i + <span class="hljs-number">1</span>}</span> Return: <span class="hljs-subst">{episode_return}</span>'</span>) <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>

  average_return = np.mean(return_list)
  print(<span class="hljs-string">f'Average Return: <span class="hljs-subst">{average_return}</span>'</span>)

  plt.plot(range(len(return_list)), return_list)
  plt.xlabel(<span class="hljs-string">'Episodes'</span>)
  plt.ylabel(<span class="hljs-string">'Returns'</span>)
  plt.title(<span class="hljs-string">f'A2C on <span class="hljs-subst">{env_name}</span>, Average Return: <span class="hljs-subst">{average_return}</span>'</span>)
  plt.savefig(<span class="hljs-string">'../A2C_results/A2C_testing.png'</span>)
  plt.show()

  env.close()
</div></code></pre>
</li>
</ul>
<h4 id="%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C">训练结果</h4>
<ul>
<li>
<p>训练结果如下：</p>
<p><img src="A2C_results/A2C_Training.png" alt="alt text"></p>
<p><strong>可以看到，在训练300次左右首次达到峰值，最后在大约500次后收敛到500。</strong></p>
</li>
<li>
<p>测试结果如下：</p>
<p><img src="A2C_results/A2C_Testing.png" alt="alt text"></p>
<p><strong>可以看到，测试结果平均返回结果达到497.52，表现优秀。个别episode会突然下降。</strong></p>
</li>
</ul>
<h4 id="%E6%80%BB%E7%BB%93">总结</h4>
<p>注意到，A2C的收敛速度不如PPO和REINFORCE。但收敛效果和稳定性都比DQN好。A2C在处理连续动作空间和高维状态空间时表现优秀，适用于多种连续控制和组合优化任务。</p>

</body>
</html>
